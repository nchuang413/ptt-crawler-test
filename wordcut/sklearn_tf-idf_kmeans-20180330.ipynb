{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd # 引用套件並縮寫為 pd  \n",
    "\n",
    "from jieba.analyse import *\n",
    "\n",
    "filename='ptt20180315-3yv2utf8.csv'\n",
    "datapath = os.getcwd() + '\\\\'+ filename\n",
    "setdictpath = os.getcwd() + '\\\\'+ 'dict.txt.big'\n",
    "userdictpath  = os.getcwd() + '\\\\'+ 'user_dict2.txt'\n",
    "stopword_path= os.getcwd() + '\\\\'+ 'stopword_tw3.txt'\n",
    "#outputpath = os.getcwd() + '\\\\'+ 'output_tfidf.csv'\n",
    "\n",
    "#dictionary path assign\n",
    "jieba.set_dictionary(setdictpath)\n",
    "jieba.load_userdict(userdictpath) \n",
    "\n",
    "#content as a list\n",
    "def sectencelist(corpus):\n",
    "    sentence = []\n",
    "    for s in corpus :\n",
    "        sentence.append(s)\n",
    "    return sentence\n",
    "\n",
    "# make stopword list\n",
    "def stopwordslist(stopword_path):  \n",
    "    stopwords = [line.strip() for line in open(stopword_path, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords \n",
    "\n",
    "#去除數字  標點符號\n",
    "def symbolremove(list):\n",
    "    sentence_re = []\n",
    "    for s in list:\n",
    "        s = re.sub(r'[A-Za-z]+','', s)\n",
    "        s = re.sub(r'\\d+', '', s)\n",
    "        s = re.sub('<[^>]*>', '', s)\n",
    "        s = re.sub('[\\W]+', '', s)\n",
    "        sentence_re.append(s)\n",
    "    return sentence_re\n",
    "\n",
    "#用 jieba 斷詞\n",
    "def clearText(text):\n",
    "    mywordlist = []\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    liststr=\"/ \".join(seg_list)\n",
    "    f_stop = open(stopword_path, encoding='utf-8')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read( )\n",
    "        #f_stop_text=unicode(f_stop_text,'utf-8')\n",
    "    finally:\n",
    "        f_stop.close( )\n",
    "    f_stop_seg_list=f_stop_text.split('\\n')\n",
    "    for myword in liststr.split('/'):\n",
    "        if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "            mywordlist.append(myword)\n",
    "    return ''.join(mywordlist)   \n",
    "    \n",
    "    \n",
    "\n",
    "df = pd.read_csv(datapath ,encoding='UTF-8')\n",
    "\n",
    "corpus = df.values[:,3]\n",
    "\n",
    "step1 = sectencelist(corpus)\n",
    "step2 = symbolremove(step1)\n",
    "step3_lists =[]\n",
    "for i in range(0,len(step2)):\n",
    "    step3_list=clearText(step2[i])\n",
    "    step3_lists.append(step3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopword_path,encoding='utf8') as f:\n",
    "    stop_list = [i.strip() for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text  import  TfidfTransformer  \n",
    "from  sklearn.feature_extraction.text  import  CountVectorizer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(step3_lists)\n",
    "word = vectorizer.get_feature_names()\n",
    "tfidf = transformer.fit_transform(X) \n",
    "weight = tfidf.toarray()\n",
    "final_data_list=[]\n",
    "for i in range(len(weight)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重  \n",
    "    #print (u\"-------这里输出第\",i,u\"类文本的词语tf-idf权重------\" )\n",
    "    #print(weight)\n",
    "    data_list=[]\n",
    "    for j in range(len(word)):\n",
    "        data={}\n",
    "        data['word']=word[j]\n",
    "        data['weight']=weight[i][j]\n",
    "        data_list.append(data)  \n",
    "    sort_list=sorted(data_list, key=lambda k: k['weight'],reverse=True)\n",
    "    #sort words by weight\n",
    "    clean_data_list=[]\n",
    "    for words in sort_list[0:30]:\n",
    "        clean_data={}\n",
    "        if words['weight'] == 0:\n",
    "            break\n",
    "        clean_data['word']=words['word']\n",
    "        clean_data['weight']=words['weight'].astype(type('float', (float,), {}))\n",
    "        clean_data_list.append(clean_data)\n",
    "    final_data_list.append(clean_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "for i in range(len(step3_lists)):\n",
    "    data={}\n",
    "    data['words']=final_data_list[i]\n",
    "    data['id']=df.values[i,0]\n",
    "    data_list.append(data)\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words_list=[]\n",
    "for i in data_list:\n",
    "    tag_words=[]\n",
    "    for i in i['words']:\n",
    "        tag_words.append(i['word'])\n",
    "    tag_words_list.append(' '.join(tag_words))\n",
    "tag_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_list=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text  import  TfidfTransformer  \n",
    "from  sklearn.feature_extraction.text  import  CountVectorizer\n",
    "transformer = TfidfTransformer() \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tag_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "distortions = []\n",
    "for i in range(1, 15):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    result=km.fit_predict(X)\n",
    "    km.inertia_\n",
    "    print('predict : ',result)\n",
    "plt.plot(range(1, 15), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=11, \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300, \n",
    "            random_state=0)\n",
    "km.fit(X)\n",
    "distortions.append(km.inertia_)\n",
    "result=km.fit_predict(X)\n",
    "print('predict : ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words_list=0\n",
    "distortions=0\n",
    "cut_article_comment_list=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_list=[]\n",
    "for i in range(len(data_list)):\n",
    "    data={}\n",
    "    data['group'] = result[i].astype(type('int', (int,), {}))\n",
    "    data['id']= df.values[i,0]\n",
    "    data['words']=data_list[i]['words']\n",
    "    new_data_list.append(data)\n",
    "new_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export result to csv file\n",
    "outputpath = os.getcwd() + '\\\\'+ 'kmeansgrouping_result.csv'\n",
    "\n",
    "with open(outputpath, 'w', encoding='UTF-8') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=stop_list)\n",
    "X = vectorizer.fit_transform(step3_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "k = 15\n",
    "km = KMeans(n_clusters=k, random_state=2)\n",
    "km.fit(X)\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "label_count = Counter(km.labels_)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Topic %d (Count: %d):\" % (i, label_count[i]), end=\" \")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print('%s ' % terms[ind], end=\" \")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    word_score = []\n",
    "    for ind, score in zip(order_centroids[i, :10],  km.cluster_centers_[i, ::-1]):\n",
    "        word_score.append((terms[ind], score))\n",
    "        \n",
    "    word_wc = WordCloud(font_path=\"data/msjh.ttf\").generate_from_frequencies(''.join(word_score))\n",
    "    plt.imshow(word_wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic %d (Count: %d):\" % (i, label_count[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "distortions = []\n",
    "for i in range(1, 15):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    result=km.fit_predict(X)\n",
    "    km.inertia_\n",
    "    print('predict : ',result)\n",
    "plt.plot(range(1, 15), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如何選擇 k\n",
    "\n",
    "#實務上我們讓程式幫忙選擇一個適合的 k，使得群間差異最大、群內差異最小\n",
    "\n",
    "from sklearn import cluster, datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 迴圈\n",
    "silhouette_avgs = []\n",
    "ks = range(2, 20)\n",
    "for k in ks:\n",
    "    kmeans_fit = cluster.KMeans(n_clusters = k).fit(X)\n",
    "    cluster_labels = kmeans_fit.labels_\n",
    "    silhouette_avg = metrics.silhouette_score(X, cluster_labels)\n",
    "    silhouette_avgs.append(silhouette_avg)\n",
    "\n",
    "# 作圖並印出 k = 2 到 20 的績效\n",
    "plt.bar(ks, silhouette_avgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silhouette_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由績效圖我們發現 在2群、3群的績效比較好，因此我們可以將K設定為2 OR 3\n",
    "\n",
    "km = KMeans(n_clusters=2)  #K=2群\n",
    "y_pred = km.fit_predict(X)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Rate of working')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred) #C是第三維度 已顏色做維度\n",
    "plt.show()\n",
    "\n",
    "km.cluster_centers_ #各群中心點(X,Y)的位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用文字雲來做視覺化\n",
    "#!pip install wordcloud\n",
    "for i in range(k):\n",
    "    word_score = []\n",
    "    for ind, score in zip(order_centroids[i, :10],  km.cluster_centers_[i, ::-1]):\n",
    "        word_score.append((terms[ind], score))\n",
    "\n",
    "word_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
